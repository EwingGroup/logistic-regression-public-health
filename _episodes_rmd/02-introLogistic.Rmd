---
source: Rmd
title: "An introduction to logistic regression"
objectives:
  - "Identify questions that can be addressed with a logistic regression model."
  - "Describe the components that are involved in logistic regression."
  - "Formulate the model equation in terms of the log odds."
  - "Formulate the model equation in terms of the probability of success."
keypoints:
questions:
teaching: 10
execises: 10
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
source("../bin/obtain_data.R")
knitr_fig_path("01-")
```

Logistic regression is commonly used, but when is it appropriate to apply this method? Broadly speaking, logistic regression may be suitable when the following conditions hold:

* You seek a *model* of the relationship between one *binary* dependent variable and one *continuous* or *categorical* explanatory variable. 
* Your data and logistic regression model do not violate the assumptions of the logistic regression model. We will cover these assumptions in the final episode of this lesson.

> ## Exercise  
> A colleague has started working with the NHANES data set. They approach 
> you for advice on the use of logistic regression on this data. 
> Assuming that the assumptions of the logistic regression model hold, which
> of the following questions could potentially be tackled with a logistic 
> regression model? Think closely about the outcome and explanatory
> variables, between which a relationship will be modelled to answer the 
> research questions.
>   
> A) Does home ownership (whether a participant's home is owned or rented)
> vary across income bracket in the general US population?  
> B) Is there an association between BMI and pulse in the general US population?  
> C) Do participants with diabetes on average have a higher weight than
participants without diabetes?  
> 
> > ## Solution
> > A) The outcome variable is home ownership and the explanatory variable
> > is income bracket.
> > Since home ownership is a binary outcome variable, this question could
> > be suited for logistic regression.    
> > B) Since both variables are continuous, this question is not suitable
> > for logistic regression.  
> > C) The outcome variable is weight and the explanatory variable is
> > diabetes. Since the outcome variable is continuous and 
> > the explanatory variable is binary, 
> > this question is not suited for logistic regression.
> > Note that an alternative question, with diabetes as the outcome 
> > variable and weight as the explanatory variable, could be suited
> > for logistic regression. 
> {: .solution}
{: .challenge}


The logistic regression model can be described by the following equation:

$$
\text{log}\left(\frac{E(y)}{1-E(y)}\right) = \beta_0 + \beta_1 \times x_1.
$$

The right-hand side of the equation has the same form as what we encountered in the simple linear regression lesson. So we will first interpret the left-hand side of the equation. The outcome variable is denoted by $y$. Logistic regression models the *log* of the *ratio* of the *expectation* of $y$ and $1$ minus the *expectation* of $y$. This log ratio can be denoted by $\text{logit()}$, giving us the following equation:

$$
\text{logit}(E(y)) = \beta_0 + \beta_1 \times x_1.
$$

As we learned in the previous episode, the expectation of $y$ is another way of referring to the *mean* of $y$. We also learned that $E(y) = \text{Pr}(y=1)$ and that $1- \text{Pr}(y=1) = \text{Pr}(y=0)$. Therefore, the left-hand side of our equation can be denoted by:

$$
\text{logit}(E(y)) = \text{log}\left(\frac{E(y)}{1-E(y)}\right) = \text{log}\left(\frac{\text{Pr}(y=1)}{1-\text{Pr}(y=1)}\right) = \text{log}\left(\frac{\text{Pr}(y=1)}{\text{Pr}(y=0)}\right). 
$$

This leads us to interpreting $\text{logit}(E(y))$ as the *log odds* of $y=1$.

The expectation of $y$ is a function of $\beta_0$ and $\beta_1 \times x_1$. The intercept is denoted by $\beta_0$ - this is the log odds when the explanatory variable, $x_1$, equals 0. The effect of our explanatory variable is denoted by $\beta_1$ - for every one-unit increase in $x_1$, the log odds changes by $\beta_1$.


Before fitting the model, we have access to $y$ and $x_1$ values for each observation in our data. For example, we may want to model the relationship between diabetes and BMI. $y$ would represent diabetes ($y=1$ if a participant has diabetes and $y=0$ otherwise). $x_1$ would represent BMI. After we fit the model, R will return to us values of $\beta_0$ and $\beta_1$ - these are *estimated* using our data. 



> ## Exercise  
> We are asked to study the association between BMI and diabetes. We are given the following equation of a logistic regression model to use:  
> $$\text{logit}(E(y)) = \beta_0 + \beta_1 \times x_1$$.    
> Match the following components of this logistic regression model to their descriptions:  
> 1. $\text{logit}(E(y))$  
> 2. ${\beta}_0$  
> 3. $x_1$
> 4. ${\beta}_1$  
>  
> A) The log odds of having diabetes, for 
> a particular value of BMI.   
> B) The expected change in the log odds of having diabetes
> with a one-unit increase in BMI.     
> C) The expected log odds of
> having diabetes when the BMI equals 0.  
> D) A specific value of BMI.  
>  
> > ## Solution
> > A) 1   
> > B) 4  
> > C) 2  
> > D) 3  
> {: .solution}
{: .challenge}

Alternatively, the logistic regression model can be expressed in terms of probabilities of success. This formula is obtained by using the *inverse* function of $\text{logit}()$, denoted by $\text{logit}^{-1}()$. In general terms, an inverse function "reverses" the original function, returning the input value. This means that $\text{logit}^{-1}(\text{logit}(E(y))) = E(y)$. Taking the inverse logit on both sides of the logistic regression equation introduced above, we obtain:

$$
\begin{align}
\text{logit}^{-1}(\text{logit}(E(y))) & = \text{logit}^{-1}(\beta_0 + \beta_1 \times x_1) \\
E(y) & = \text{logit}^{-1}(\beta_0 + \beta_1 \times x_1).
\end{align}
$$

The advantage of this formulation is that our output is in terms of probabilities of success. We will encounter this formulation when plotting the results of our models. 

> ## Exercise  
> We are asked to study the association between age and smoking status. We are given the following equation of a logistic regression model to use:  
> $$E(y) = \text{logit}^{-1}(\beta_0 + \beta_1 \times x_1).$$     
> Match the following components of this logistic regression model to their descriptions:  
> 1. $E(y)$  
> 2. ${\beta}_0$  
> 3. $x_1$
> 4. ${\beta}_1$  
> 5. $\text{logit}^{-1}()$
>  
> A) A specific value of age.  
> B) The expected probability of being a smoker for a particular
> value of age.  
> C) The inverse logit function.  
> D) The linear predictor for smoking given an age of 0.  
> E) The expected change in the linear predictor with a one-unit
> difference in age. 
>  
> > ## Solution
> > A) 3   
> > B) 1  
> > C) 5  
> > D) 2  
> > E) 4
> {: .solution}
{: .challenge}


