---
source: Rmd
title: "Logistic regression with one continuous explanatory variable"
objectives:
  - "Use the `ggplot2` package to explore the relationship between a binary response variable and a continuous explanatory variable."
  - "Use the `glm()` function to fit a logistic regression model with one continuous explanatory variable."
  - "Use the `summ()` function from the `jtools` package to interpret the model output in terms of the log odds."
  - "Use the `summ()` function from the `jtools` package to interpret the model output in terms of the odds ratio."
  - "Use the `jtools` and `ggplot2` packages to visualise the resulting model."
keypoints:
questions:
teaching: 10
execises: 10
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
source("../bin/obtain_data.R")
knitr_fig_path("03-")

library(dplyr)
library(tidyr)
library(ggplot2)
library(jtools)
```

In this episode we will learn to fit a logistic regression model when we have one binary response variable and one continuous explanatory variable. Before we fit the model, we can explore the relationship between our variables graphically. We are checking whether, on average, observations split along the binary variable appear to differ in the explanatory variable.

Let us take response variable `SmokeNow` and the continuous explanatory variable `Age` as an example. For participants that have smoked at least 100 cigarettes in their life, `SmokeNow` denotes whether they still smoke. The code below drops NAs in the response variable. The plotting is then initiated using `ggplot()`. Inside `aes()`, we select the response variable with `y = SmokeNow` and the continuous explanatory variable with `x = Age`. Finally, violin plots are called using `geom_violin()`.

The plot suggests that on average, participants of younger age are still smoking and participants of older age have given up smoking. We can now proceed with fitting the logistic regression model. 

```{r explore SmokeNow_Age}
dat %>%
  drop_na(SmokeNow) %>%
  ggplot(aes(y = SmokeNow, x = Age)) +
  geom_violin() 
```

> ## Exercise  
> You have been asked to model the relationship between 
> physical activity (`PhysActive`)
> and `FEV1` in the NHANES data. Use the `ggplot2`
> package to create an exploratory plot, ensuring that:
> 1. NAs are discarded from the `PhysActive` variable.  
> 2. Physical activity (`PhysActive`) on the y-axis and FEV1 
> (`FEV1`) on the x-axis.
> 3. This data is shown as a violin plot.  
> 4. The y-axis is labelled as "Physically active".
>
> > ## Solution
> > 
> > ```{r explore PhysActive_FEV1, warning = FALSE}
> > dat %>%
> >   drop_na(PhysActive) %>%
> >   ggplot(aes(x = FEV1, y = PhysActive)) +
> >   geom_violin() +
> >   ylab("Physically active")
> > ```
> {: .solution}
{: .challenge}

We fit the model using `glm()`. As with the `lm()` command, we specify our response and explanatory variables with `formula = SmokeNow ~ Age`. In addition, we specify `family = "binomial"` so that a logistic regression model is fit by `glm()`.  

```{r fit SmokeNow_Age}
SmokeNow_Age <- dat %>%
  glm(formula = SmokeNow ~ Age, family = "binomial")

summ(SmokeNow_Age, exp = TRUE)
```

The logistic regression model equation associated with this model has the general form:

$$\text{logit}(E(y)) = \beta_0 + \beta_1 \times x_1.$$

Recall that $\beta_0$ estimates the log odds when $x_1 = 0$ and $\beta_1$ estimates the difference in the log odds associated with a one-unit difference in $x_1$. Using `summ()`, we can obtain estimates for $\beta_0$ and $\beta_1$:

```{r summ SmokeNow_Age}
summ(SmokeNow_Age, digits = 3)
```

The equation therefore becomes:

$$\text{logit}(E(\text{PhysActive})) = 2.607 - 0.054 \times \text{Age}.$$

Alternatively, we can express the model equation in terms of the probability of "success": 

$$\text{Pr}(y = 1) = \text{logit}^{-1}(\beta_0 + \beta_1 \times x_1).$$

In this example, $\text{SmokeNow} = \text{Yes}$ is "success". The equation therefore becomes:

$$\text{Pr}(\text{SmokeNow} = \text{Yes}) = \text{logit}^{-1}(2.607 - 0.054 \times \text{Age}).$$

> ## Exercise  
> 1. Using the `glm()` command, fit a logistic regression model
> of physical activity (`PhysActive`) as a function of FEV1 
> (`FEV1`).
> Name this `glm` object `PhysActive_FEV1`.  
> 2. Using the `summ` function from the `jtools` package, answer the following questions:
>   
> A) What log odds of physical activity does the model predict, 
> on average, for an individual with an `FEV1` of 0?  
> B) By how much is the log odds of physical activity expected 
> to differ, on average, for a one-unit difference in `FEV1`?  
> C) Given these values and the names of the response and explanatory
> variables, how can the general equation $\text{logit}(E(y)) = \beta_0 + \beta_1 \times x_1$ be adapted to represent the model?  
> D) By how much is $\frac{\text{Pr}(\text{PhysActive}}{\text{Pr}(\text{PhysActive}} = \text{No})$ expected to be multiplied for a one-unit increase in `FEV1?`
>
> > ## Solution
> > 
> > To answer questions A-C, we look at the default
> > output from `summ()`:
> > 
> > ```{r fit PhysActive_FEV1 - part 1}
> > PhysActive_FEV1 <- dat %>%
> >   drop_na(PhysActive) %>%
> >   glm(formula = PhysActive ~ FEV1, family = "binomial")
> > 
> > summ(PhysActive_FEV1, digits = 5)
> > ```
> > 
> > A) -1.1860     
> > B) The log odds of physical activity is expected 
> > to be 0.0005 for every unit increase in `FEV1`.      
> > C) $\text{logit}(E(\text{PhysActive})) = -1.1860 + 0.00005 \times \text{FEV1}$.  
> > 
> > To answer question D, we add `exp = TRUE` to the `summ()`
> > command:
> > 
> > ```{r fit PhysActive_FEV1 - part 2}
> > summ(PhysActive_FEV1, digits = 5, exp = TRUE)
> > ```
> > 
> > D) The multiplicative change in the odds of
> > physical activity being "Yes" is estimated to be 1.00046. 
> {: .solution}
{: .challenge}

Finally, we can visualise our model using the `effect_plot()` function from the `jtools` 
package. Importantly, logistic regression models are often visualised in terms of 
the probability of success, i.e. $\text{Pr}(\text{SmokeNow} = \text{Yes})$ in our example.

We specify our model inside `effect_plot()`, alongside our explanatory variable of interest
with `pred = Age`. To aid interpretation of the model, we include the original data points
with `plot.points = TRUE`. Recall that our data is binary, so the data points are exclusively
$0$s and $1$s. To avoid overlapping points becoming hard to interpret, we add jitter using
`jitter = c(0.1, 0.05)` and opacity using `point.alpha = 0.1`. 

```{r plot SmokeNow_Age}
effect_plot(SmokeNow_Age, pred = Age, plot.points = TRUE,
            jitter = c(0.1, 0.05), point.alpha = 0.1)
```

> ## Exercise  
> To help others interpret the `PhysActive_FEV1` model, produce a figure. 
> Make this figure using the `jtools` package, ensuring that 
> the y-axis is labelled as "Pr(PhysActive = Yes)".
>
> > ## Solution
> > ```{r plot PhysActive_FEV1}
> > effect_plot(PhysActive_FEV1, pred = FEV1, plot.points = TRUE,
> >             jitter = c(0.1, 0.05), point.alpha = 0.1) +
> >   ylab("Pr(PhysActive = Yes)")
> > ```
> {: .solution}
{: .challenge}


