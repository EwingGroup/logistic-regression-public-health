---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-introLogistic.md in _episodes_rmd/
source: Rmd
title: "An introduction to linear regression"
objectives:
  - "Identify questions that can be addressed with a logistic regression model."
  - "Describe the components that are involved in logistic regression."
keypoints:
questions:
teaching: 10
execises: 10
---



Logistic regression is commonly used, but when is it appropriate to apply this method? Broadly speaking, logistic regression may be suitable when the following conditions hold:

* You seek a *model* of the relationship between one *binary* dependent variable and one *continuous* or *categorical* explanatory variable. 
* Your data and logistic regression model do not violate the assumptions of the logistic regression model. We will cover these assumptions in the final episode of this lesson.

> ## Exercise  
> A colleague has started working with the NHANES data set. They approach 
> you for advice on the use of logistic regression on this data. 
> Assuming that the assumptions of the logistic regression model hold, which
> of the following questions could potentially be tackled with a logistic 
> regression model? Think closely about the outcome and explanatory
> variables, between which a relationship will be modelled to answer the 
> research questions.
>   
> A) Does home ownership (whether a participant's home is owned or rented)
> vary across income bracket in the general US population?  
> B) Is there an association between BMI and pulse in the general US population?  
> C) Do participants with diabetes on average have a higher weight than
participants without diabetes?  
> 
> > ## Solution
> > A) The outcome variable is home ownership and the explanatory variable
> > is income bracket.
> > Since home ownership is a binary outcome variable, this question could
> > be suited for logistic regression.    
> > B) Since both variables are continuous, this question is not suitable
> > for logistic regression.  
> > C) The outcome variable is weight and the explanatory variable is
> > diabetes. Since the outcome variable is continuous and 
> > the explanatory variable is binary, 
> > this question is not suited for logistic regression.
> > Note that an alternative question, with diabetes as the outcome 
> > variable and weight as the explanatory variable, could be suited
> > for logistic regression. 
> {: .solution}
{: .challenge}









The logistic regression model can be described by the following equation:

$$
\frac{E(y)}{1-E(y)} = \beta_0 + \beta_1 \times x_1
$$.

The right-hand side of the equation has the same form as what we encountered in the simple linear regression lesson. So we will first interpret the left-hand side of the equation. The outcome variable is denoted by $y$. Logistic regression models the ratio of the *expectation* of $y$ and $1$ minus the *expectation* of $y$. This ratio can be denoted by $\text{logit()}$, giving us the following equation:

$$
\text{logit}(E(y)) = \beta_0 + \beta_1 \times x_1
$$

The expectation of $y$ is another way of referring to the *mean* of $y$. When $y$ is a binary variable, its mean is a probability of success, i.e. $E(y) = \text{Pr}(y=1)$. Since probabilities add up to one, $1- \text{Pr}(y=1) = \text{Pr}(y=0)$. Therefore, the left-hand side of our equation can be denoted by:

$$
\text{logit}(E(y)) = \frac{E(y)}{1-E(y)} = \frac{\text{Pr}(y=1)}{1-\text{Pr}(y=1)} =  \frac{\text{Pr}(y=1)}{\text{Pr}(y=0)}
$$

This leads us to interpreting $\text{logit}(E(y))$ as the *odds* of $y=1$. For example, let's say we model tomorrow's weather, with $y=1$ if it is sunny and $y=0$ if it is not. We may have an odds of 3 if $\frac{\text{Pr}(y=1)}{\text{Pr}(y=0)} = \frac{0.75}{0.25} = 3$. We would have an odds of 1 if it is equally likely to have a sunny or a non-sunny day, i.e. $\frac{\text{Pr}(y=1)}{\text{Pr}(y=0)} = \frac{0.5}{0.5} = 1$. 

The expectation of $y$ is a function of $\beta_0$ and $\beta_1 \times x_1$. The intercept is denoted by $\beta_0$ - this is the value of $E(y)$ when the explanatory variable, $x_1$, equals 0. The effect of our explanatory variable is denoted by $\beta_1$ - for every one-unit increase in $x_1$, $E(y)$ changes by $\beta_1$.


Before fitting the model, we have access to $y$ and $x_1$ values for each observation in our data. For example, we may want to model the relationship between weight and height. $y$ would represent weight and $x_1$ would represent height. After we fit the model, R will return to us values of $\beta_0$ and $beta_1$ - these are *estimated* using our data. 



> ## Exercise  
> We are asked to study the association between BMI and diabetes. We are given the following equation of a logistic regression model to use:  
> $$\text{logit}(E(y)) = \beta_0 + \beta_1 \times x_1$$.    
> Match the following components of this logistic regression model to their descriptions:  
> 1. $\text{logit}(E(y))$  
> 2. ${\beta}_0$  
> 3. $x_1$
> 4. ${\beta}_1$  
>  
> A) The logarithm of the odds of having diabetes, for 
> a particular value of BMI.   
> B) The expected change in the logarithm of the odds of having diabetes
> with a one-unit increase in BMI.     
> C) The expected logarithm of the odds of
> having diabetes when the BMI equals 0.
> D) A specific value of BMI.  
>  
> > ## Solution
> > A) 1   
> > B) 4  
> > C) 2  
> > D) 3  
> {: .solution}
{: .challenge}

An alternative way of writing the same simple linear regression model is:

$$
y_i = \beta_0 + \beta_1 \times {x_1}_i + \epsilon_i
$$.

The following elements have changed:
* Instead of $E(y)$, we have $y_i$. This is the observed value of the response for participant $i$.
* Instead of $x_1$, we have ${x_1}_i$. This is the observed value of the explanatory variable for participant $i$. 
* In addition, we have $\epsilon_i$. This is also known as the *residual* for participant $i$. The residual equals the difference between the outcome expected for participant $i$ by the model and the true observed value, i.e. $\epsilon_i = E(y_i) - y_i$. We will return to residuals in the final episode of this lesson, where they will play a role in assessing the assumptions of the simple linear regression model. 

> ## Exercise  
> Staying with our model of the effect of participant's age on their BMI, 
> we are now given the alternative equation of a simple linear regression to use: 
> $$y_i = \beta_0 + \beta_1 \times {x_1}_i + \epsilon_i$$.    
> Match the following components of this simple linear regression model to their
> descriptions:  
> 1. $y_i$  
> 2. ${\beta}_0$  
> 3. ${x_1}_i$
> 4. ${\beta}_1$
> 5. $\epsilon_i$
>  
>    
> A) The mean change in BMI following a one-unit increase in age.   
> B) The mean BMI when age equals 0.      
> C) A participant's BMI.  
> D) The difference between the expected and observed BMI values.  
> E) A participant's age.  
>  
> > ## Solution
> > A) 4   
> > B) 2  
> > C) 1  
> > D) 5  
> > E) 3
> {: .solution}
{: .challenge}

